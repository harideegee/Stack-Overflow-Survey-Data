{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Removing Duplicates**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will focus on data wrangling, an important step in preparing data for analysis. Data wrangling involves cleaning and organizing data to make it suitable for analysis. One key task in this process is removing duplicate entries, which are repeated entries that can distort analysis and lead to inaccurate conclusions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will perform the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Identify duplicate rows  in the dataset.\n",
    "2. Use suitable techniques to remove duplicate rows and verify the removal.\n",
    "3. Summarize how to handle missing values appropriately.\n",
    "4. Use ConvertedCompYearly to normalize compensation data.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Installing collected packages: tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.4.0 pandas-2.3.3 tzdata-2025.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset into a DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the dataset using pd.read_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ResponseId                      MainBranch                 Age  \\\n",
      "0           1  I am a developer by profession  Under 18 years old   \n",
      "1           2  I am a developer by profession     35-44 years old   \n",
      "2           3  I am a developer by profession     45-54 years old   \n",
      "3           4           I am learning to code     18-24 years old   \n",
      "4           5  I am a developer by profession     18-24 years old   \n",
      "\n",
      "            Employment RemoteWork   Check  \\\n",
      "0  Employed, full-time     Remote  Apples   \n",
      "1  Employed, full-time     Remote  Apples   \n",
      "2  Employed, full-time     Remote  Apples   \n",
      "3   Student, full-time        NaN  Apples   \n",
      "4   Student, full-time        NaN  Apples   \n",
      "\n",
      "                                    CodingActivities  \\\n",
      "0                                              Hobby   \n",
      "1  Hobby;Contribute to open-source projects;Other...   \n",
      "2  Hobby;Contribute to open-source projects;Other...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                             EdLevel  \\\n",
      "0                          Primary/elementary school   \n",
      "1       Bachelor’s degree (B.A., B.S., B.Eng., etc.)   \n",
      "2    Master’s degree (M.A., M.S., M.Eng., MBA, etc.)   \n",
      "3  Some college/university study without earning ...   \n",
      "4  Secondary school (e.g. American high school, G...   \n",
      "\n",
      "                                           LearnCode  \\\n",
      "0                             Books / Physical media   \n",
      "1  Books / Physical media;Colleague;On the job tr...   \n",
      "2  Books / Physical media;Colleague;On the job tr...   \n",
      "3  Other online resources (e.g., videos, blogs, f...   \n",
      "4  Other online resources (e.g., videos, blogs, f...   \n",
      "\n",
      "                                     LearnCodeOnline  ... JobSatPoints_6  \\\n",
      "0                                                NaN  ...            NaN   \n",
      "1  Technical documentation;Blogs;Books;Written Tu...  ...            0.0   \n",
      "2  Technical documentation;Blogs;Books;Written Tu...  ...            NaN   \n",
      "3  Stack Overflow;How-to videos;Interactive tutorial  ...            NaN   \n",
      "4  Technical documentation;Blogs;Written Tutorial...  ...            NaN   \n",
      "\n",
      "  JobSatPoints_7 JobSatPoints_8 JobSatPoints_9 JobSatPoints_10  \\\n",
      "0            NaN            NaN            NaN             NaN   \n",
      "1            0.0            0.0            0.0             0.0   \n",
      "2            NaN            NaN            NaN             NaN   \n",
      "3            NaN            NaN            NaN             NaN   \n",
      "4            NaN            NaN            NaN             NaN   \n",
      "\n",
      "  JobSatPoints_11           SurveyLength SurveyEase ConvertedCompYearly JobSat  \n",
      "0             NaN                    NaN        NaN                 NaN    NaN  \n",
      "1             0.0                    NaN        NaN                 NaN    NaN  \n",
      "2             NaN  Appropriate in length       Easy                 NaN    NaN  \n",
      "3             NaN               Too long       Easy                 NaN    NaN  \n",
      "4             NaN              Too short       Easy                 NaN    NaN  \n",
      "\n",
      "[5 rows x 114 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the URL of the dataset\n",
    "file_path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\"\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to ensure it loaded correctly\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: If you are working on a local Jupyter environment, you can use the URL directly in the <code>pandas.read_csv()</code>  function as shown below:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Identifying Duplicate Rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1: Identify Duplicate Rows**\n",
    "  1. Count the number of duplicate rows in the dataset.\n",
    "  2. Display the first few duplicate rows to understand their structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile duplicates (['MainBranch', 'Employment', 'RemoteWork']): 64876\n",
      "\n",
      "First few profile duplicates:\n",
      "                                          MainBranch  \\\n",
      "0                     I am a developer by profession   \n",
      "1                     I am a developer by profession   \n",
      "2                     I am a developer by profession   \n",
      "3                              I am learning to code   \n",
      "4                     I am a developer by profession   \n",
      "5                        I code primarily as a hobby   \n",
      "6  I am not primarily a developer, but I write co...   \n",
      "7                              I am learning to code   \n",
      "8                        I code primarily as a hobby   \n",
      "9                     I am a developer by profession   \n",
      "\n",
      "                                          Employment RemoteWork  \\\n",
      "0                                Employed, full-time     Remote   \n",
      "1                                Employed, full-time     Remote   \n",
      "2                                Employed, full-time     Remote   \n",
      "3                                 Student, full-time        NaN   \n",
      "4                                 Student, full-time        NaN   \n",
      "5                                 Student, full-time        NaN   \n",
      "6                                Employed, full-time     Remote   \n",
      "7  Student, full-time;Not employed, but looking f...        NaN   \n",
      "8                                Employed, full-time  In-person   \n",
      "9  Independent contractor, freelancer, or self-em...     Remote   \n",
      "\n",
      "                                             Country                 Age  \n",
      "0                           United States of America  Under 18 years old  \n",
      "1  United Kingdom of Great Britain and Northern I...     35-44 years old  \n",
      "2  United Kingdom of Great Britain and Northern I...     45-54 years old  \n",
      "3                                             Canada     18-24 years old  \n",
      "4                                             Norway     18-24 years old  \n",
      "5                           United States of America  Under 18 years old  \n",
      "6                           United States of America     35-44 years old  \n",
      "7                                         Uzbekistan     18-24 years old  \n",
      "8  United Kingdom of Great Britain and Northern I...     45-54 years old  \n",
      "9                                             Serbia     35-44 years old  \n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "profile_cols = ['MainBranch', 'Employment', 'RemoteWork']\n",
    "profile_dups = df.duplicated(subset=profile_cols).sum()\n",
    "print(f\"Profile duplicates ({profile_cols}): {profile_dups}\")\n",
    "\n",
    "profile_mask = df.duplicated(subset=profile_cols, keep=False)\n",
    "print(\"\\nFirst few profile duplicates:\")\n",
    "print(df[profile_mask][profile_cols + ['Country', 'Age']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Removing Duplicate Rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2: Remove Duplicates**\n",
    "   1. Remove duplicate rows from the dataset using the drop_duplicates() function.\n",
    "2. Verify the removal by counting the number of duplicate rows after removal .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows removed: 64876\n",
      "Profile duplicates remaining: 0\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "df_clean = df.drop_duplicates(subset=profile_cols, keep='first').reset_index(drop=True)\n",
    "print(f\"Rows removed: {len(df) - len(df_clean)}\")\n",
    "\n",
    "remaining_dups = df_clean.duplicated(subset=profile_cols).sum()\n",
    "print(f\"Profile duplicates remaining: {remaining_dups}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Handling Missing Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3: Identify and Handle Missing Values**\n",
    "   1. Identify missing values for all columns in the dataset.\n",
    "   2. Choose a column with significant missing values (e.g., EdLevel) and impute with the most frequent value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 columns with most missing values:\n",
      "                            Missing_Count  Missing_%\n",
      "AINextMuch less integrated            539      96.08\n",
      "AINextLess integrated                 532      94.83\n",
      "ConvertedCompYearly                   461      82.17\n",
      "CompTotal                             450      80.21\n",
      "AINextNo change                       446      79.50\n",
      "AINextMuch more integrated            428      76.29\n",
      "Knowledge_7                           427      76.11\n",
      "Frequency_3                           426      75.94\n",
      "ProfessionalTech                      426      75.94\n",
      "Knowledge_5                           425      75.76\n",
      "EdLevel missing before: 0\n",
      "Most frequent EdLevel: 'Bachelor’s degree (B.A., B.S., B.Eng., etc.)'\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "missing_counts = df_clean.isnull().sum()\n",
    "missing_percent = (missing_counts / len(df_clean)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_counts,\n",
    "    'Missing_%': missing_percent\n",
    "}).round(2)\n",
    "\n",
    "print(\"Top 10 columns with most missing values:\")\n",
    "print(missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False).head(10))\n",
    "\n",
    "edlevel_missing_before = df_clean['EdLevel'].isnull().sum()\n",
    "print(f\"EdLevel missing before: {edlevel_missing_before}\")\n",
    "\n",
    "most_frequent_edlevel = df_clean['EdLevel'].mode()\n",
    "if len(most_frequent_edlevel) > 0:\n",
    "    mode_value = most_frequent_edlevel[0]\n",
    "    print(f\"Most frequent EdLevel: '{mode_value}'\")\n",
    "    df_clean['EdLevel'] = df_clean['EdLevel'].fillna(mode_value)\n",
    "    edlevel_missing_after = df_clean['EdLevel'].isnull().sum()\n",
    "else:\n",
    "    print(\"No non-null EdLevel values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Normalizing Compensation Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4: Normalize Compensation Data Using ConvertedCompYearly**\n",
    "   1. Use the ConvertedCompYearly column for compensation analysis as the normalized annual compensation is already provided.\n",
    "   2. Check for missing values in ConvertedCompYearly and handle them if necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compensation statistics:\n",
      "count       100.000000\n",
      "mean      52522.040000\n",
      "std       92498.439046\n",
      "min           1.000000\n",
      "25%        5749.250000\n",
      "50%       30611.000000\n",
      "75%       63233.250000\n",
      "max      803285.000000\n",
      "Name: ConvertedCompYearly, dtype: float64\n",
      "\n",
      "Missing values: 461 (82.2% of 561 rows)\n",
      "\n",
      "Shape after dropping missing compensation: (100, 114)\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "print(\"Compensation statistics:\")\n",
    "print(df_clean['ConvertedCompYearly'].describe())\n",
    "\n",
    "missing_comp = df_clean['ConvertedCompYearly'].isnull().sum()\n",
    "total_rows = len(df_clean)\n",
    "missing_pct = (missing_comp / total_rows) * 100\n",
    "\n",
    "print(f\"\\nMissing values: {missing_comp} ({missing_pct:.1f}% of {total_rows} rows)\")\n",
    "\n",
    "df_final = df_clean.dropna(subset=['ConvertedCompYearly']).reset_index(drop=True)\n",
    "print(f\"\\nShape after dropping missing compensation: {df_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Summary and Next Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this lab, you focused on identifying and removing duplicate rows.**\n",
    "\n",
    "- You handled missing values by imputing the most frequent value in a chosen column.\n",
    "\n",
    "- You used ConvertedCompYearly for compensation normalization and handled missing values.\n",
    "\n",
    "- For further analysis, consider exploring other columns or visualizing the cleaned dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Dataset Preview:\n",
      "                                          MainBranch  \\\n",
      "0                     I am a developer by profession   \n",
      "1  I am not primarily a developer, but I write co...   \n",
      "2  I am not primarily a developer, but I write co...   \n",
      "3  I am not primarily a developer, but I write co...   \n",
      "4                     I am a developer by profession   \n",
      "\n",
      "                                          Employment  \\\n",
      "0  Employed, full-time;Student, full-time;Indepen...   \n",
      "1  Independent contractor, freelancer, or self-em...   \n",
      "2  Student, full-time;Independent contractor, fre...   \n",
      "3             Employed, full-time;Student, full-time   \n",
      "4  Independent contractor, freelancer, or self-em...   \n",
      "\n",
      "                                             EdLevel  \\\n",
      "0  Secondary school (e.g. American high school, G...   \n",
      "1  Some college/university study without earning ...   \n",
      "2       Bachelor’s degree (B.A., B.S., B.Eng., etc.)   \n",
      "3       Bachelor’s degree (B.A., B.S., B.Eng., etc.)   \n",
      "4                                     Something else   \n",
      "\n",
      "                    Country  ConvertedCompYearly  \n",
      "0                  Pakistan               7322.0  \n",
      "1                     Italy             107406.0  \n",
      "2                     Italy               4833.0  \n",
      "3  United States of America             136000.0  \n",
      "4                   Germany              55851.0  \n",
      "Dataset ready for advanced analysis and visualization!\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "\n",
    "print(\"Clean Dataset Preview:\")\n",
    "print(df_final[['MainBranch', 'Employment', 'EdLevel', 'Country', 'ConvertedCompYearly']].head())\n",
    "\n",
    "print(\"Dataset ready for advanced analysis and visualization!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "2116052544ce403759eef2159eb3d21f1d38e895d652bcaffa36a5791482361d"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
